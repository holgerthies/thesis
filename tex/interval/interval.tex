%!TEX root = ../../thesis.tex
\section{Interval Arithmetic}
	When using floating point arithmetics several real numbers are mapped to the
  same floating point representation, thus in most cases the number will not be
  represented exactly.

	Further, a single floating point number does not contain any information
  about its accuracy, and so it is impossible to know how big the error is.

	Often, one wants to at least bound the error. 
    
  An easy way to do this, is using interval arithmetic, i.e. instead of making
  computations on isolated single real numbers, whole intervals are used.

	This means a real number $r \in \RR$ is represented by an interval $x := [x_l,x_r]$, so that $r \in x$.
	The advantage to floating point arithmetics is that the length of the interval gives a bound on the rounding error, 
	thus making it possible to guarantee the accuracy of computed results.

	The fundamental property of interval arithmetic is the inclusion property:

	Every $f: \RR \to \RR$ can be extended to a function $\bar f$ on intervals, such that for all $x \in [a,b]$, $f(x) \in \bar f[a,b]$. 

	The interval extension of a function is not unique, but usually one tries to
  find a mapping that is as tight as possible, i.e. the length of the resulting interval
  should be small.
  
  Basic arithmetic operations can be easily defined on intervals:
	\begin{theorem}
		Let $x = [x_l, x_r]$ and $y = [y_l, y_r]$ then it holds
		\begin{eqnarray}
			x + y  & = & [x_l + y_l, x_r + y_r] \\
			x - y  & = & [x_l - y_r, x_r + y_l] \\
			x \times y  & = & [\min(x_ly_l, x_ly_r, x_ry_l, x_ry_r), \max(x_ly_l, x_ly_r, x_ry_l, x_ry_r)] \\
			\frac{1}{x} & = & \left[\frac{1}{x_r}, \frac{1}{x_l} \right] \text{ if } x_l > 0 \text{ or } x_r < 0 \\
			\frac{x}{y} & = & x \times \frac{1}{y}   
		\end{eqnarray}
	\end{theorem}
	It is easy to see, that addition and multiplication are associative and commutative.
	However, the following example shows, that the distributive law does not necesserily hold
	\begin{example}
		\begin{align*}
			[-1,1] \times ([-1,0] + [3,4])  &= [-1,1] \times [2,4] = [-4,4] \\
			[-1,1] \times [-1,0] + [-1,1] \times [3,4] &=  [-1,1] + [-4,4] =[-5,5] 
		\end{align*}
	\end{example} 
	Thus, the length of the resulting interval is not independent of the way the
  computation is done.
	
	The next theorem provides a simple way to find interval extensions of many functions.
	\begin{theorem}
		For every monotonic function $f: \RR \to \RR$ it holds
		$$ f(x) = [\min(f(x_l), f(x_r)), \max(f(x_l), f(x_r))] $$
		In particular this can be used to find interval extensions of piecewise monotonic functions, such as
		\begin{equation}
			x^n  =   
				\begin{cases} 
					[x_l^n, x_r^n] &\mbox{if } n \mbox{ is even or } x_l \geq 0 \\
					[x_r^n, x_l^n] &\mbox{if n is odd and } x_r \leq 0 \\
					[0, \max(x_r^n, x_l^n)] & \mbox{otherwise.}  \\
				\end{cases} 
		\end{equation} 
	\end{theorem}
	In a similiar way, interval version for $\exp, \log, \sin, \cos$ and other elementary functions can be found, 
	that can then be used to find the interval versions of more complicated functions.

	The strength of interval arithmetic is, that it can easily be implemented even when the above 
	operations on the interval endpoints are not computed exactly, but finite approximations are used.
	
	For that to work, the only thing that is important, is that outward rounding is used, i.e. 
	after every operation the left point of the interval has to be rounded down and the right point rounded up.

	IEEE 754 requires, that the user is able to specify the rounding mode, thus interval arithmetic can be implemented 
	using standard floating point numbers.

	Note that the length of the interval can increase quickly.

	There are many implementations of interval arithmetic in different programming languages, e.g. 
	for \cc the boost interval arithmetic library can be used.

  Interval arithmetic can be combined with arbitrary-precision arithmetic to
  get arbitrarily small intervals, leading to arbitrarily exact computations
  with the possibility to bound the rounding error after the computation.

