%!TEX root = ../../thesis.tex
\section{Floating Point Arithmetic}
The traditional way to compute with real numbers on a computer is the use 
of \textbf{floating point arithmetic}. 

The floating point representation of a real number is a bit-string with a  fixed
length.

Numbers can be represented the following way:
\begin{definition}\label{def: floating point number}
	A floating point representation with base $b$ and precision $p$
	is a string $\pm d_0 . d_1 \dots d_{p-1} \times b^e$ with $0 \leq d_i < b$.	
	It represents the number
	$$ \pm b^e \cdot \sum_{i=0}^{p-1} d_i\beta^{-i} $$ 

	$\pm d_0 . d_1 \dots d_{p-1}$ is called the Mantissa and $e$ the exponent.
	
	The smallest and largest exponent allowed are $e_min$ and $e_max$.
	A floating point number can be encoded in
	$$ \lceil log_2(e_{max}-e_{min}+1) \rceil + \lceil  p \cdot log_2 (b) \rceil + 1 $$
	bits.
\end{definition}
If a real number does not fit into the above representation, it has to be
rounded appropriately.

Rounding also happens, when an operation (e.g. multiplication) is applied and
the result does not fit into the finite representation anymore.

With the above definition the floating point representation of a number is not
unique. 
To guarantee uniqueness it is usually required that the representation  is normalized, i.e. that $d_0 \neq 0$. 

Of course, $0$ can not be represented in this form, so often a special
representation is chosen for $0$.

Doing real number computations using floating point numbers will in most cases
lead to rounding errors.
One can distinguish two cases of errors, the \textbf{absolute error}, the
distance between the real result and the computed one, and the \textbf{relative
error}, the absolute error divided by the real result.

Floating point arithmetic usually works by
operating on
only a fixed number of digits for computation.
This makes computations fast, but can lead to large relative errors as in the following theorem: 
\begin{theorem}
	The relative error when computing $x-y$ for floating point numbers $x,y$ can be as large as $b-1$. 
\end{theorem}
Note: Guard digits
\subsection{The IEEE Standard}
	Floating point representations for binary base are standardized by IEEE 754 \cite{ieee}. 
	This standard is followed by almost all modern computers.
	The standard defines four different precisions: single, single-extended, double and double-extended.
	  \begin{table}
	  	\centering
	    \begin{tabular}{ | c || c | c | c | c | c | }
	    \hline
	    Type & $p$ & $e_{max}$ & $e_{min}$ & exponent width (bits) & format width (bits) \\ \hline \hline
	    Single & 24 & +127 & -126 & 8 & 32 \\ \hline
	    Single-Extended & 32 & +1023 & $\leq -1022$ & $\leq 11$ & 43 \\ \hline
	    Double & 53 & +1023 & -1022 & 11 & 64 \\ \hline
	    Double-Extended & 64 & > 16383 & $\leq -16382$ & 15 & 79 \\ \hline
	    \end{tabular}
	    \caption{Parameters for the different types in the IEEE 754 standard}\label{table:IEEE floating point}
	  \end{table}
	  
	The details can be seen in Table \ref{table:IEEE floating point}.
	The standard requires that the result of addition, subtraction, multiplication and division is exactly rounded, 
	i.e. the operation has to be performed exactly and then rounded.

	Floating point arithmetic is not necessarily associative, e.g.

	The standard also introduces special quantities $-0, +0, \inf, +\inf$ and NAN (not a number). 

	Rounding modes

	


