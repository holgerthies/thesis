%!TEX root = ../../thesis.tex
\section{Real Complexity Theory}\label{section:real_complexity}
	\subsection{Classical Complexity Theory}
    In contrast to computability theory, complexity theory deals with the
    question how many resources (in form of e.g. time or space) are needed to
    compute a computable function.

    Again, the Turing-Machine model is used to describe the computations
		\begin{definition}
			For a given Turing Machine $M$ and $w \in \Sigma^*$, $time_M(w)$ is the number of head movements 
			the Turing Machine on input $w$ exectues before it terminates. \\
			For $n \in \NN$ define $time_M(n) = \max \{ time_M(w) \,|\, w \in \Sigma^* and M terminates on input w \}$.\\
			Analogously, one can define space constraints.
		\end{definition}
    In most cases it is not important to compute the exact running time, but
    one rather wants to approximate how the algorithm will behave when the
    input is getting large.
    The standard way to compare the asymptotic running time of algorithms is
    the use of $O$-notation.
		\begin{definition}
			For functions $f, g: \NN \to \NN$ one writes $f \in O(g(n))$ if there are constants $M \in \RR$, $n_0 \in \NN$, such that
			$ f(n) \leq M \cdot g(n)$ for all $n > n_0$. 
		\end{definition}
			Thus, for a given algorithm a way to measure its complexity is giving an
      upper bound on its worst case running time for inputs of length $n$ in
      terms of the $O$-notation. 

			However, complexity theory mainly is not about the complexity of specific
      algorithms, but rather deals with the complexity of problems. 
      That is, trying to classify the complexity of \textbf{any} algorithm to
      decide a subset of $\Sigma^*$.
      
      The following gives a classifications of problems depending on the
      running time for algorithms solving those problems.
		\begin{definition}
			For a function $t: \NN \to \NN$ let 
      $$ \text{DTIME}(t) = \{ A \subseteq \Sigma^* \,|\, \text{There is a
      Turingmachine } M \text{ deciding } A \text{ with time}_M(n) \in O(t(n)) \} $$

      NTIME$(t)$ describes the same for the case of $M$ being a
      non-deterministic Turing-machine.
		\end{definition}
		The above definitions suffice to define the most important complexity classes
		\begin{definition}
			The complexity classes $P, NP, PSPACE$ and $EXPTIME$ are defined as follows:
			\begin{eqnarray*}
				P & := & \bigcup_{k \in N} DTIME(n^k) \\
				NP & := & \\bigcup_{k \in N} NTIME(n^k) \\
				PSPACE & := & \bigcup_{n \in N} DSPACE(n^k) \\
				EXPTIME & := & \bigcup_{k \in N} DTIME(2^{n^k}) \\
			\end{eqnarray*}
		\end{definition}
		It holds $P \subseteq NP \subseteq PSPACE \subseteq EXPTIME$. 
		It is $P \neq EXPTIME$ and for all other inclusions it is not known if equality holds, but most 
		complexity theorists consider it as extremely unlikely.

    Another way to characterize the important class $NP$ is the following
    \begin{theorem}
      $A \subseteq \Sigma^*$ is in $NP$ if and only if there are polynomials
      $p$ and $q$ and a deterministic Turing-Machine $M$ such that for all $x
      \in \Sigma^*$ with length n, there is an $y \in \Sigma*$ with length
      bounded by $q(n)$ such that $x \in A \text{ iff } M(<x,y>) = 1$ and 
      $x \not \in A \text{ iff } M(<x,y>) = 0$. 
      Further $M$'s running time is bounded by $p(n)$ for all such inputs
      $<x,y>$.  
    \end{theorem}
		Wheter $P = NP$ is one of the Millenium problems and one of the biggest unsolved problems in Computer Science.

		One can also consider function problems instead of decision problems
    leading to a dual hierarchy of complexity classes.
		\begin{definition}
      A functional problem $F: \Sigma^* \rightrightarrows \Sigma^*$, is solved
      by a Turing-Machine in if it writes on every $x \in \Sigma^*$ an output
      $z \in F(x)$ or decides that no such output exists.  \\ Time- and
      Space-Complexity can be defined accordingly. \\ The classes $FP$ and
      $FPSPACE$ are defined as the class of polynomial time resp. space
      computable functions. \\ The class $\#P$ is defined as the the class of
      functions that give the number of solutions to a problem in $NP$.
    \end{definition}
		It holds $FP \subseteq \#P \subseteq FPSPACE$ and if $FP = \#P$ would hold, it would follow that $P = NP$.
     
		Problems that are known to be in $P$ are often considered as the feasible functions, 
		for which it is possible to find an efficient algorithm. 

    Since most of the complexity classes are not known to be distinct, they do
    not suffice to classify problems by their difficulty.
    To compare the difficulty of problems, reductions are used.
		\begin{definition}
			$A \subset \Sigma^*$ is \textbf{polynomial time reducible} to $B \subseteq \Sigma^*$ ($A \leq_P B)$, 
			if there is a polynomial time computable function $f; \Sigma^* \to \Sigma^*$, such that
			$$ w \in A \Leftrightarrow f(w) \in B.$$
		\end{definition}
		Many other types of reductions exist, e.g. to compare complexity classes
    lower then $P$, but they are not needed in this thesis and therefore
    omitted.

		The hardest problems in a complexity class are called complete for this class
		\begin{definition}
			A set $A \subseteq \Sigma^*$ is called \textbf{$\C$-hard} (w.r.t. $\leq_P$) for a complexity class $\C$, if for all $B \in \C$ $B \leq_P A$. \\
			$A$ is called \textbf{$\C$-complete} (w.r.t. $\leq_P$), if $A$ is $\C-hard$ and $A \in \C$.  
		\end{definition}
		If one would show for a single $\C$-complete problem that it is in $P$, it would follow that all problems in $\C$ are in $P$.
		Consequently a way to show that there is most likely no efficient algorithm to a problem, is to show that the problem is $NP$-hard.
    For many problems it could be shown that they are $NP$-complete. 
    Some of the most important ones can for example be found in
    \cite{bookwithnpcompleteproblems}
	\subsection{Complexity Theory on real numbers}
		In classical complexity theory the time complexity is usually measured in terms of the input size.
		However, when considering continuous peoblems, the input is an infinite string and thus can not 
		be used to measure the running time of an algorithm.
		In numerical computations an important parameter is the desired output precision.
		Thus, it seems reasonable to analyze the running time as a function depending on the desired output precision.
		The complexity of a real number can be defined as follows:
		\begin{definition}\label{def:complexity_real_number}
			A real number $x \in \RR$ is computable in time $t$ if there is a Turing Machine $M$ that with input $n \in \NN$ in binary, 
			outputs the binary expansion of a dyadic rational number $d$ with $| x - d | \leq 2^{-n}$.  
		\end{definition}
		Again this notion can be generalized by using representations, i.e. by considering the time that is needed to write the first $n$ symbols
		of the name of the output.
		One has to be careful, however, since representations that are computationally equivalent, do not necessary lead to the same complexity bounds.
		For example, one can construct arbitrarily long Cauchy names for any rational number, so that it is not even possible to define the complexity 
		of a real number by its Cauchy name.
		A representation that is often considered is the signed digit representation
		\begin{definition}
			The \textbf{signed digit representation} $\rho_sd$ is defined as follows: \\
			A $\rho_d$-name of a real number $x$ is a sequence 
			$$a_n a_{n-1} \dots a_0 . a_{-1} a_{-2} \dots \text{ with } a_i \in \{-1,0,1\}, n \geq -1, a_n \neq 0, a_n + a_{n-1} \neq 0$$
			$$ \text{ and } x = \sum_{i=n}^{-\infty} a_i \cdot 2^i $$  
		\end{definition}
		As with the binary expansion, the number digits after the binary point correspond to the precision.
		The advantage to the binary expansion is that, the signed digit representation is symmetric, ,,,
		It can be shown that the signed digit representation is computably equivalent to the Cauchy representation.
		Definition \ref{def:complexity_real_number} can be adjusted to the signed digit representation in the obvious way.\\
		When extending the complexity notion to functions, an additional parameter might be interesting, the precision of the input
		needed to compute the output up to the demanded precision.
		It should also be noted, that the definition of complexity is only possible on compact sets, since otherwise the machine can already
		spend arbitrarily much time to read the digits before the point.
		\begin{definition}
			A function $f: \subseteq \RR \to \RR$ is computed on a compact set $K \subseteq dom(f)$ in time $t$ with lookahead $l$ if for all $\rho_{sd}$-names of elements $ x \in K$ by a Turing Machine $M$ if $M$ computes a $\rho_{sd}$-name of $f(x)$ and after t steps and with reading at most $l$ symbols of the input string outputs the $n$-th symbol of the output.   
		\end{definition} 
		\begin{theorem}[Ko and Friedmann]
			The set of polynomial time computable real numbers forms a real algebraically closed field.
		\end{theorem}
		\begin{example}
		Polynomial time computable functions...
		\end{example}
	\subsection{Complexity of Operators}
		Since most interesting function spaces such as $C[0,1]$ are not locally compact,
		there is no straight forward way to generalize the above definitions to get a uniform notion of complexity on such spaces.
		In fact, it holds
		\begin{theorem}
			Even restricted to continuous functions $f: [0,1] \to [0,1]$, the evaluation operator $x \to f(x)$ is not computable within time uniformly bounded in terms of the output error only.
		\end{theorem}
		The approach by Ko and Friedman \cite{KoBook} is to study the time complexity of the function $F(f)$ for polynomial time computable $f$.
		This  often leads to a connection between discrete complexity classes and numerical problems, as in the following theorem:
		\begin{theorem}[Friedman \cite{Fri}]
			The following are equivalent
			\begin{enumerate}
				\item $FP = \#P$
				\item For every polynomial time computable function $f: [0,1] \to \R$, the function
					$g: [0,1] \to \R$, defined by $g(x) = \int_0^x f(t) dt$ is polynomial time computable.
			\end{enumerate} 
		\end{theorem}
		In this sense, one can say that Integration is $\#P$-hard.

		Second order representations
 	\subsection{Parameterized Complexity}
 		Often it is not possible to bound the running time in terms of the output precision only.
 		In other cases, having some additional information about the input can increase the running time dramatically,
 		e.g. going from $NP$-complete to polynomial time computable.
 		The notion of enriching a representation with additional information can be formalized as follows...
	
